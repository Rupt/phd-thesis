\begin{singlespacing}
\chapter{Searches in ATLAS}
\label{chapter:searches}
%
\begin{epigraphs}
\qitem{%
One way is to make it so simple that there are \emph{obviously} no deficiencies
and the other way is to make it so complicated that there are no \emph{obvious}
deficiencies.%
}%
{Tony~Hoare,
\textit{The Emperorâ€™s Old Clothes},
1981~\cite{hoare2007emperor}}
\end{epigraphs}
\end{singlespacing}

Our task in scientific data analysis is to distil an informative
message from data and to report that message to the wider community.
From any given data there are many possible messages that could be extracted,
and those messages will have different values to different recipients of our
messages --- our data analysis is not an automatic process, but a result
of many judgements that are informed by theory, convention, and intuition.

To clarify our later description of the $\twoljets$ search in
Chapter~\ref{chapter:2ljets}, this chapter introduces the basic theory and
practice of data analysis and its manifestations in the \atlas\ SUSY community.

Theoretical data analysis in the tightly coupled fields of probability theory
and decision theory is introduced in Section~\ref{sec:searces_data_analysis}.
An attempted alternative formulation, frequentist theory, is discussed in
Section~\ref{sec:searches_frequentist} for its relevance to our methods.
Standard practice in this field of research does not strictly follow
frequentist methods, but patches up some of their pathologies with modified
approaches that we describe in Section~\ref{sec:searches_practice}.
Searches for Supersymmetric signals in \atlas\ follow a standard strategy for
their data analysis; its procedures, nomenclature, and parametric modelling
are introduced in Section~\ref{sec:searches_searches}.


\section{Data analysis}
\label{sec:searces_data_analysis}
How many jelly beans are in the jar of Figure~\ref{fig:significant_beans}?
Your answer depends not only on what you observe from the figure, but also on
what else you know about cartoon jars of beans, and on what reward is on offer.
Each answer is a decision that depends both on what the possible rewards are,
and on which numbers of beans you subjectively infer to be likely.
We shall return to this example after developing some probability and decision
theory.

\begin{figure}[tp]
\centering
\includegraphics[width=0.25\textwidth]{figures/searches_beans.pdf}
\caption[
How many beans are in the jar?
]{%
How many beans are in the jar? This jar of jelly beans is a vehicle for our
discussion of inference and decision theory.
Please write your guess on this page.
The first correct answer wins a gold star.
}
\label{fig:significant_beans}
\end{figure}

Statistical data analysis suffers from dogma --- prescriptions and recipes that
students are taught to memorize and follow without knowing their reasons or
limitations.
From a standard undergraduate physics education, my experience has been
error propagation formulae,
least-squares fitting routines,
chi-squared per degree of freedom,
and myriad statistical tests and confidence intervals.
Many of these are effective approximations, but all also fail if misused
outside their domain.%
\footnote{%
Multiplicative error propagation is an example.
For $a = \mu_a \pm \sigma_a$ and $b = \mu_b \pm \sigma_b$,
the perturbative approximation is
\(
\widetilde{\sigma}_{ab}
= \mu_a \mu_b \sqrt{\sigma_a^2/\mu_a^2 + \sigma_b^2/\mu_b^2}
% = \sigma_a^2 \mu_b^2 + \sigma_b^2 \mu_a^2
\),
but calculation with their moments derives
\(
\sigma_{ab}
= \sqrt{(\mu_a^2 + \sigma_a^2)(\mu_b^2 + \sigma_b^2) - \mu_a^2\mu_b^2}
% = \sigma_a^2 \mu_b^2 + \sigma_b^2 \mu_a^2 + \sigma_a^2 \sigma_b^2
\)
exactly if independent.
Since $\sigma_{ab}^2 = \widetilde{\sigma}_{ab}^2 + \sigma_a^2 \sigma_b^2$, the
propagation formula works only for small relative errors.
}
All effective methods of data analysis can be derived upwards from the roots of
probability and decision theory, and as elsewhere in physics, by knowing the
derivations we can also know when each approximation should work and when it
needs replacing.

We begin with what probability is: at its most abstract, a probability a
proportion of stuff within a larger whole.
Probabilities useful because proportions have applications, and because they
can be manipulated and interrelated with a linear and intuitive algebra.
Probabilisitc data analysis follows the ordinary structure of science ---
write down models that make predictions, test those predictions against data,
then update your models to refine their predictions.
For this process, likelihoods are particularly important because they describe
our predictions. But not everything is a likelihood.
The remainder of this section uses a standard example from High Energy Physics
to introdule this probabilistic structure and other parts of its technical
language.

\paragraph{Probability, abstract.}
A probability $\prob{b}{a}$ is a number that quantifies the proportion
of $b$ within $a$, and can be expressed as a ratio~\cite{axioms1010038}
\begin{equation}
\label{eqn:searches_prob_ratio}
\prob{b}{a} = \frac{m(b, a)}{m(a)},
\end{equation}
in which $m(x)$ is a measure (a non-negative quantity assigned to its
argument, such as mass of tea in a cup), and the comma `,' means `and'
(logical and or set intersection or any other equivalent operation).
To continue that example, $\prob{b}{a}$ could be a proportion of tea within
a spoon within a cup, where $m(b, a)$ measures the volume of tea in a
teaspoon that is partially submerged in the cup.%
\footnote{%
All probabilities are conditional in this definition; $\probc{b}$ alone does
not exist unless the conditioning information $a$ is implicit from context.%
}

Logic is an important application of probability for which $a$ and $b$ are
interpreted as logical propositions, and
$\prob{b}{a}$ is the degree to which $a$ implies $b$, with the limiting cases
that $\prob{b}{a} = 1$ when $a$ implies $b$ and $\prob{b}{a} = 0$ when $a$
disproves $b$.
Indeed, Cox's theorem derives the laws of probability by requiring compatibility
with Boolean logic~\cite{
cox1946probability,
cox1961algebra,
garrett1998nand,
jaynes2003probability,
keynes1920treatise
}.
Probability works for logic, and it works \emph{for tea, too}.

Two algebraic rules derive from Equation~\ref{eqn:searches_prob_ratio}:
a \textbf{product rule}
\begin{align}
\prob{c, b}{a} &= \prob{c}{b, a}\times \prob{b}{a},
\label{eqn:searches_product_rule}
\intertext{%
that describes nested proportions
(perhaps a spoon within a teacup within a sink),
and a \textbf{sum rule}%
}
\prob{c\vee b}{a} &= \prob{c}{a} + \prob{b}{a} - \prob{c, b}{a},
\label{eqn:searches_sum_rule}
\end{align}
that describes combination in a union $\vee$ that coincides with logical `or'.
The subtraction avoids double-counting of any overlap, and is often avoided by
splitting any problem into orthogonal, or disjoint states.
For disjoint cases with labels $b_i$, normalization therefore requires that
\begin{equation}
\label{eqn:searches_normalization}
\sum_i \prob{b_i}{a} = 1.
\end{equation}
These rules are basic and well known, but we state them here to support the
following statements of how probabilities can be applied for practical
results.%
\footnote{%
The formulation with sums and products is not strictly unique, but all
alternatives are equivalent up to an invertible
transformation~\cite{axioms1010038}.
This choice is usually best because it uses ordinary addition and
multiplication.
In numerical analysis, however, it is often practical to use $\log$
probabilities to avoid overflows outside the range representable by floating
point numbers, thus changing multiplication to addition and addition to
the `$\log$-add-$\exp$' operation.
}

\paragraph{Probability, applied.}
% science snake
Science involves making predictions, testing those predictions against data,
and reporting what is learned from the results.
Various probabilities are involved in this prediction and learning that have
conventional names to communicate what we intend to do with them.
Most important for prediction is a \textbf{likelihood}
\begin{align}
L(a) &= \prob{b}{a} \\
\label{eqn:searches_likelihood}
\intertext{%
that explores the probabilities assigned to a fixed result $b$ from different
contexts $a$, or how well different hypotheses predicted a result.
A likelihood is clearly a probability, but it is not a probability distribution
--- only the left hand argument $b$ forms a normalized distribution.
Alternatively, fixing the context gives a \textbf{prior}%
}
\pi(b) &= \prob{b}{a}
\label{eqn:searches_prior}
\end{align}
for fixed $a$, which is a normalized distribution over disjoint states $b$.

Likelihood and prior are evidently related --- they explore two indices into
the same object, two sides of the same coin, and are assigned by the same
principles.
But they are profoundly different objects that work for different purposes,
and clarity in their distinction will help in our later understanding of
practical methods.
For observed data, we use likelihoods to compare which hypotheses predicted
them better than others, and before observing data we use priors to describe
what data might result from different models to design our experiments.

To illustrate this, consider a histogram that bins event yields in
$\met$, with distributions from background and (supersymmetric) signal samples
overlaid.
Example such histograms are displayed in
Figure~\ref{fig:searches_sig_bkg_prior_likelihood}.
Since these histograms are normalized to unit area, they show prior
probability distributions along the bin axis; there is one prior
for events from each of the background and signal hypotheses.
Priors distribute unit probability explore along the bin axis.
In each bin, however, a likelihood function compares the assignments from
the two hypotheses: signal and background.
Likelihoods explore vertically down the sample axis.

\begin{figure}[tp]
\centering
\includegraphics[width=\textwidth]{figures/searches_sig_bkg_prior_likelihood.pdf}
\caption[
Background and signal prior densities
]{%
Background and signal prior densities in $\eV[G]$ and $\eV[G]^{1/2}$ units.
When changing coordinates, probability mass moves as a fluid of constant total
volume, changing its shape in this density view to keep the area of the
rectangle in each bin constant.
A likelihood function indexes each bin in the background--signal labels,
and is independent of the coordinates units since the area in each bin is
invariant.
}
\label{fig:searches_sig_bkg_prior_likelihood}
\end{figure}

Identical distributions are drawn in two different coordinates
on the left and right of Figure~\ref{fig:searches_sig_bkg_prior_likelihood};
they are identical in content, but differ in appearance because they use the
standard approach of representing each prior distribution as a density.%
\footnote{%
Density is a somewhat misleading term here.
Probability mass does not compress, it moves and redistributes --- it is more
like depth of tea than density of air.%
}
To preserve the area of each bin, which shows its probability mass,
bin heights therefore change to adapt to their widths that change due to the
differing values of $\met$ and $\sqrt{\met}$.
This coordinate-dependence of the prior density means that its maximum is
not always meaningful.
Since the probability mass in each bin is constant, the likelihood is
coordinate-independent.

One typically approximates such histograms by Monte Carlo simulation methods,
since simulations do not generate bin indices but event variables that derive
from the sampled events.
To connect these concept with probabilities, we begin with some notation;
we have hypotheses $h$ (signal or background),
a parameter $x$ that we histogram ($x = \met$),
and we observe a datum $d$ that datum indexes the histogram bins --- supposing
an event is observed in the overflow bin, it says
$d = `x > 100\,\eV[G]\textrm{'}$.
This observed bin has a likelihood function
\begin{equation}
L(x) = \prob{d}{x, h} = \prob{d}{x} =
\left\{
\begin{matrix}
1 & \textrm{if }~x > 100\,\eV[G], \\
0 & \textrm{otherwise} \\
\end{matrix}
\right.
\end{equation}
that corresponds to Equation~\ref{eqn:searches_likelihood} by
$b \leftarrow d$ and $a \leftarrow `x, h\textrm{'}$,
and simplifies $\prob{d}{x, h} = \prob{d}{x}$ since such binning is independent
of hypothesis.%
\footnote{%
Independence of the likelihood function from prior hypotheses is so common
that it should usually be assumed unless otherwise stated.
}
This example of a binary selection highlights how the likelihood acts as a
filter that rejects events that are contradicted by the data.
Other likelihoods that are not binary act similarly, but as inefficient filters
that reject events with some probability.

Simulating events from background and signal hypotheses samples from their
respective priors
\begin{equation}
\pi(x) = \prob{x}{h}
\end{equation}
that correspond to Equation~\ref{eqn:searches_prior} by
$b \leftarrow x$ and $a \leftarrow h$.
Although we previously used the same labels $a$ and $b$, those are only
labels --- the words `prior' and `likelihood' state our intent to index into
the left and right arguments respectively.

When we fill a histogram by summing weights of simulated samples, we are
calculating another likelihood over hypotheses $h$ alone, not $`x, h\textrm{'}$
jointly.
To disambiguate the two, we call this abstracted, $x$-independent likelihood
the \textbf{evidence} $Z(h)$ for hypothesis $h$.
The evidence is calculable as
\begin{align}
Z
= \prob{d}{h} &= \sum_i L(x_i) \,\pi(x_i)
\label{eqn:searches_evidence_integral_sum}
\\
&= \int\! L(x) \,\mathrm{d}\pi(x),
\label{eqn:searches_evidence_integral_measure}
\end{align}
where Equation~\ref{eqn:searches_evidence_integral_measure} is notation for
integrating the likelihood function over the prior measure~\cite{
billingsley2008probability,
skilling2010foundations
}, and relates most closely to our practice of simulating from the model and
counting events that hit the non-zero likelihood in the bin.
We shall shortly derive this identity.
The name `evidence' relates to its application for model
comparison~\cite{mackay2003information}.
Supposing we have this one event with $\met > 100\,\eV[G]$, did it come from
the background or signal sample?
The answer depends on the signal and background cross-sections in our collider,
but larger evidence for signal does influence any conclusion to move in its
favour.

This expression integral, or average over the prior, means that evidence $Z$
for model arises from a larger fraction of samples from it hitting large values
of the likelihood function.
In contrast, one might consider a maximum likelihood approach that constrained
an optimization to regions of nonzero prior in each model.
Such optimizations can give a useful upper bound on $Z$, but this example
highlights a flaw since both models support the $\met > 100\,\eV[G]$ bin
and both support the same maximum likelihood of
$L(\varepsilon + 100\,\eV[G]) = 1$.

% posterior
Perhaps we now want to see where in the overflow bin this event landed, and
compare the signal and background distributions there.
This is an inspection of the \textbf{posterior}
\begin{equation}
p(x) = \prob{x}{d, h} = \frac{L(x)\,\pi(x)}{Z},
\end{equation}
which is a prior probability distribution specialized by the data;
it simply uses the likelihood to filter simulated samples
(perhaps in `skimming an ntuple')
and re-normalizes them to a probability distribution within the constraint.

We finally have the four parts of `Bayesian' data analysis:
the two inputs \textbf{likelihood} and \textbf{prior},
and two outputs \textbf{posterior} and \textbf{evidence}.
These relate through Bayes' rule, which is a consequence of the
produce rule of Equation~\ref{eqn:searches_product_rule} and the
commutativity of `and' --- that `$a$ and $b$' is equivalent to
`$b$ and $a$';
the joint probability of data and parameters has two identical
forms, $\prob{d, x}{h}$ and $\prob{x, d}{h}$, that each factor into two
pieces:
\begin{align}
\prob{d}{x, h}\times \prob{x}{h} &= \prob{x}{d, h}\times \prob{d}{h},
\intertext{all of which we have named:}
\underbrace{L(x)}_\textbf{likelihood}
\times
~\,
\overbrace{\pi(x)}^\textbf{prior}
~\,
&=
\overbrace{p(x)}^\textbf{posterior}
\times
\underbrace{Z}_\textbf{evidence}
.
% \textbf{likelihood} \times \textbf{prior}
% &= \textbf{posterior} \times \textbf{evidence} .\nonumber
\end{align}
This is (the infamous) Bayes' rule, derived as a consequence of the
symmetry of logical `and' and the product rule of probability
algebra.

How Bayes' rule \emph{should} be used in scientific data analysis is to extract
useful information from the data.
If the likelihood function itself is interpretable, then it should be reported
directly.
When it is not, perhaps due to a complicated parametric modelling, then one
should abstract those parameters away by reporting evidence values for
predictive models, which must of course be clearly stated and interpretable if
they are to be useful.
Data act only through the likelihood function, and reporting a posterior
can act to obfuscate those data.

Posterior probabilities are, however, useful when digesting data to make
inferences about the world.
Reporting likelihoods and evidence values allows readers to assign priors as
they wish.
For model comparison between background $h=B$ and signal $h=S$, this is clearly
expressed in the ratio form
% likelihood ratio
\begin{equation}
\hphantom{\quad \quad \mid\mid \mathrm{context}}
\frac{\prob{S}{d}}{\prob{B}{d}}
=
\!\!
\frac{\prob{d}{S}}{\prob{d}{B}}
\!\!\times~
\frac{\probc{S}}{\probc{B}}
\quad \quad \mid\mid \mathrm{context}
,
\end{equation}
where we use the double bar, $\mid\mid$, to refer to any contextual information
a reader might wish to include.
Whenever the evidence ratio is independent of that context, it factors out from
the prior (odds) ratio.
In this sense, likelihood (or evidence) ratios are scale factors that update
any odds ratio by the same multiplication.

The infamy of Bayes' rule stems from a fixation on the posterior distributions
from unjustified prior assignments.
Classical works by
Bayes~\cite{bayes1763lii} and
Laplace~\cite{laplace1774stigler} began this focus ---
both assume prior densities that are uniform in chosen coordinates, a practice
that can yield practical results in posterior inferences.
But to report scientific data, such practices are fairly criticized as
arbitrary when they influce the reported results.
Prior assignments are arbitrary.
You are welcome to propose any prediction in any circumstances, and the
scientific response is to test whether or not its predictions are compatible
with our observations of nature.
Our emphasis on the likelihood functions and evidence values that quantify
those predictions avoids the pitfall of seeking uninformative, neutral, or
unbiased priors by embracing priors as active and explicit predictions.
This is the modern, practical approach to Bayesian data analysis~\cite{
mackay2003information,
skilling2004nested,
skilling2006nested,
sivia2006data,
skilling2010foundations
}.

\begin{figure}[tp]
\centering
\includegraphics[width=0.65\textwidth]{figures/searches_baton_roue_bayes.jpg}
\caption[
Bayesian analysis produces two outputs: the evidence and posterior
]{%
Bayesian analysis produces two outputs: the
evidence $\prob{d}{h}$ and
posterior $\prob{x}{d,h}$ for hypothesis $h$, data $d$, and parameters $x$.
Posterior-only, semi-Bayesian methods are legitimately criticized because they
do not challenge the hypotheses that assign their priors $\prob{x}{h}$.
Since the evidence is a likelihood function over hypotheses, considering the
evidence can challenge those priors.
\\[0.5em]
Cartoon adapted from ``Baton roue'' by
Corentin~Penloup~\cite{penloup2011baton}.
}
\label{fig:searches_baton_roue_bayes}
\end{figure}

Some sources continue to define Bayesian analysis as the computation of
posterior probabilities given data, often without naming the evidence, and
expressing it only as the sum from
Equation~\ref{eqn:searches_evidence_integral_sum}
--- as just a normalizing constant~\cite{
gelman1995bayesian,
gelman2008objections,
DAgostini:1994fjx,
DAgostini:2010hil,
cowan1998statistical,
pdg2020review
}.
Bayes' rule has two outputs, and both outputs have uses.
Cutting out the evidence hamstrings Bayesian analysis, leaving only the
semi-Bayesian posterior-only approach and its clear flaws.
A bicycle is stable with two wheels and falls over if one either
is removed;
Figure~\ref{fig:searches_baton_roue_bayes} illustrates this analogy.
Although it is possible to ride a unicycle, it is a circus trick and not often
a practical tool.

The essence of this form of Probabilistic data analysis is most succinctly
summarized by Skilling~\cite{skilling2008rant}:
``we SHOULD use Bayes by assigning subjective priors and inspecting evidence
values.''
This agrees exactly with the standard practice of histogramming simulations,
as illustrated in Figure~\ref{fig:searches_sig_bkg_prior_likelihood}.
Signal and background models define priors, and their (normalized) bin yields
are the evidence values that we use to compare the models.
Finally corresponding posteriors are the normalized shapes those samples take
within the observed histogram bin, which we do not use to compare the models,
but which could in principle be used for secondary interpretations.


\paragraph{Probability assignments.}
Probability algebra manipulates probabilities, but does not state how they
should be initially assigned.
In general, such probability assignments are free and unconstrained, and might
informed by calibration data or loosely motivated opinions.
But there are examples of unambiguously derived that are commonly used in
High-Energy Physics, notably the Poisson and Gaussian distributions.
To avoid turning this thesis into a textbook, we only sketch the derivations
of these, and refer to real textbooks for their detailed
derivations~\cite{jaynes2003probability}, both of which use the exponential
identity that
\begin{equation}
\label{eqn:searches_exponential}
\exp(x) = \displaystyle \lim_{N \to \infty}
\left(1 - \frac{x}{N}\right)^N
.
\end{equation}

The Large Hadron Collider collides $40$~million proton bunches per second,
but \atlas' trigger system accepts only only around $1000$ of
those~\cite{atlas2020trigger}.
This means we have scenario with a large fixed number $N$ of trials
(bunch collisions)
and a small probability of binary acceptance (triggering),
due to a notable collision event.
The summed total number $n$ of accepted events is therefore described by the
Binomial distribution for $n$;
\begin{equation}
\mathrm{Binomial}(n\mid N, p) = p^n (1 - p)^{N - n} \times \binom{N}{n}
,
\end{equation}
which is a product of the probabilities (produce rule) in the sequence of $N$
accept/reject decisions, summed (sum rule) over all of that sequence
permutations that could have led to the total $n$.
With our extremely small trigger rate, we are interested in the limit of small
$p$ and large $N$ which result in a mean acceptance number of $\lambda = Np$.
Using the exponential identity of Equation~\ref{eqn:searches_exponential}, one
can show that this limit tends towards the Poisson
distribution~\cite{jaynes2003probability};
\begin{equation}
\label{eqn:searches_poisson}
\mathrm{Poisson}(n\mid \lambda) = \frac{\lambda^n}{n!}\exp(-\lambda)
,
\end{equation}
which is used ubiquitously in the analysis of our histograms.

The Gaussian distribution has many justifications:
as the unique distribution that is spherically symmetric and factors linearly
into independent dimensions~\cite{
jaynes2003probability,
herschel1850normal,
maxwell1860normal,
muller1959note,
marsaglia1972normal
},
as a practical approximation that permits analysis in linear algebra,
and as the maximum entropy~\cite{PhysRev.106.620} distribution for a
known mean and variance.
This maximum entropy statement means that for a distribution with known mean
and variance in a given coordinates, its best Gaussian approximation matches
those moments.\footnote{%
By `best', this means minimal Kullback-Leibler divergence
\(
H(p\leftarrow q) = \int \log(p(x)/q(x)) \,\mathrm{d}p(x)
\)
from the Gaussian approximation $q$ to the target $p$ with known moments.
Such an approximation may not be \emph{good}, but it is the best in its class.%
}
As a limiting approximation, the Gaussian form arises from products of many
functions, such as combinations of many independent measurements that multiply
their likelihood functions.
At a maximum with $x=\mu$, the first derivative of a function vanishes and its
second derivative is negative, so this product looks something like
\begin{equation}
f(x)^N = \displaystyle \lim_{N \to \infty}
(1 - \kappa (x - \mu)^2)^N,
\end{equation}
which, again by Equation~\ref{eqn:searches_exponential}, goes into the Gaussian
form
\begin{equation}
\label{eqn:searches_gaussian}
\mathrm{Gaussian}(x\mid \mu, \sigma)
\,\mathrm{d}x =
\frac{1}{\sqrt{2\pi \sigma^2}}
\exp\!\left(-\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2}\right)
\mathrm{d}x
\end{equation}
where the prefactor is for normalization to a probability distribution over
$x$, and we include the $\mathrm{d}x$ terms as reminders that it is a density
function in that coordinate.

Here, $\kappa$ and $\sigma$ both relate to the second derivative of the target
function; this relationship extends to multiple dimensions in the Hessian matrix
and its inverse, and allows one to estimate error bars from second derivatives
--- a practical tool that the methods discussed in this thesis employ.

This is not the famous Central Limit Theorem, which is related to the
distribution of sums.
But can be used to derive it: the distribution of a sum is a convolution, which
becomes a product in Fourier-transformed space, and that product can converge
to a Gaussian function by this same derivation if the mean and variance exist.
A beautiful alternative proof of the Central Limit Theorem is given by
Jaynes~\cite{jaynes2003probability}
(Section ``7.16 The central limit theorem''), and would be spoiled by a
reproduction here.


\paragraph{Decision theory.}

\TODO{}
% connection to frequentist section
Bayes' theorem is an infamous and necessary result, and it is not seriously
disputed. What has been disputed is whether probability can be applied to
logic, or any application other than the random happenings of events.


\begin{singlespacing}
\section{Frequentist theory}
\label{sec:searches_frequentist}
\begin{epigraphs}
\qitem{%
Within this theory, statistical methods of great practical usefulness have been
developed, and its statements can and frequently do contribute in a vague way
to the interpretation of data. \ldots%
}%
{John~W.~Pratt,
\textit{Review: Testing Statistical Hypotheses},
1961~\cite{pratt1961testing}}
\end{epigraphs}
\end{singlespacing}

Figure~\ref{fig:searches_significant}

\begin{figure}[tp]
\centering
\includegraphics[width=0.46\textwidth]{figures/searches_significant_shrink.png}
\\
\begin{footnotesize}
`So, uh, we did the green study again and got no link. It was probably a{-}{-}'
\\
`RESEARCH CONFLICTED ON GREEN JELLY BEAN ACNE LINK; MORE~STUDY~RECOMMENDED!'
\end{footnotesize}
\caption[
``Significant'' by Randall~Munroe
]{%
``Significant'' by Randall~Munroe~\cite{xkcd2011significant}.
}
\label{fig:searches_significant}
\end{figure}


\begin{singlespacing}
\section{Post-frequentist practice}
\label{sec:searches_practice}
\begin{epigraphs}
\qitem{%
\ldots\ But this book, by its very excellence, its thoroughness, lucidity and
precision, intensifies my growing feeling that nevertheless the theory is
arbitrary, be it however ``objective,'' and the problems it solves, however
precisely it may solve them, are not even simplified theoretical counterparts
of the real problems to which it is applied.%
}%
{John~W.~Pratt,
\textit{Review: Testing Statistical Hypotheses},
1961~\cite{pratt1961testing}}
\end{epigraphs}
\end{singlespacing}


% significance measures
The ATLAS-recommended significance measure~\cite{atlas_significance} is
\begin{align}
\label{eqn:significance_atlas}
S_\mathrm{\atlas}(n; \mu, \sigma) =~&
\sqrt{2} \times
\mathrm{sign}(n - \mu) \times
\\[0.2em] \nonumber
&
\sqrt{
n\log\left(\frac{n(\mu + \sigma^2)}{\mu^2 + n\sigma^2}\right)
- \frac{\mu^2}{\sigma^2}\log\left(
1 + \frac{\sigma^2(n - \mu)}{\mu(\mu + \sigma^2)}
\right)
}
,
\end{align}
in an appropriate limit where $0\log(0x) = 0$ for the $n=0$ case.


\section{Searches}
\label{sec:searches_searches}
% histograms, CR VR SR, systematics
% histfactory

\TODO{minimize energy not max logf}
To model the consequences of these many factors of uncertainty, each factor is
assigned a real-valued parameter, which when varied changed the expected yields
in the histogram bins of the search.
In this way, we form a statistical model which is a function
\begin{equation}
H_{\!f}(\vec \theta) \rightarrow (\vec \mu, \log f)
\end{equation}
from the parameters $\theta$ to Poisson expectations $\vec\mu$ of the histogram
bins and a regularization term $\log f$ that is maximized in fitting.

\histfactory~\cite{cranmer2012histfactory}
\pyhf~\cite{heinrich2021pyhf}
\histfitter~\cite{Besjes_2015,baak2015histfitter}


\clearpage

Hello again
