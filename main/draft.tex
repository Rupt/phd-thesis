As $\sigma \rightarrow 0$, this tends towards the the significance-transformed,
sign-flipped Poisson maximum likelihood ratio
\begin{equation}
S_\mathrm{Poisson}(n, \mu, \sigma)
= \mathrm{sgn}(n - \mu) \sqrt{2\left(
    n\log\left(\frac{n}{\mu}\right) - n + \mu
\right)}
~.
\end{equation}



\subsection{Higgs to tau-tau Nested Sampling}
Proof-of-concept implementation of the Nested Sampling algorithm~\cite{
skilling2004nested,
skilling2006nested,
skilling2010foundations
}
to classify events mediated by Higgs or Z bosons in their leptonic decays
through tau lepton pairs in \atlas.
In this scheme, Nested Sampling approximates likelihoods at given data for
approximate models of the Higgs and Z models by `marginalizing',
or integrating over, all freedoms in their production in LHC proton-proton
collisions.
Models of their processes were constructed from theory for the
helicity-dependent leptonic decays of tau leptons~\cite{Hays2014tau}
(with their two neutrinos integrated out analytically)
and Z bosons~\cite{Thomson2011EWK},
and parametrized from numerical parton distribution
functions~\cite{Buckley:2014ana} for the models' differing distributions of
longitudinal boosts.

Unlike the similar Matrix Element Method~\cite{
Fiedler:2010sg,
Gainer:2013iya,
PhysRevD.83.074010
},
which estimates likelihoods by fixing certain observed quantities and
integrating remaining freedoms with adaptive Monte Carlo algorithms~\cite{
Fiedler:2010sg,
Gainer:2013iya,
Lepage:1977sw,
Ohl:1998jn
},
with Nested Sampling we begin in ignorance of the data in each event and
gradually explore its constraints with random walks.
Euclidean coordinates projected to the surface of the unit ball,
in each of the respective rest frames of the heavy boson and tau leptons, allow
those walks to avoid the gimble lock of spherical polar coordinates, and are
used to describe Lorentz transformations to transform the observable leptons
to the lab frame.

This method worked in testing on smeared parton-level simulations from
\madgraph~\cite{Alwall:2014hca} and was presented in two conference posters.
\TODO{Should these posters be included as figures? Maybe appendix}
However, real events have substantial additional noise from QCD
radiation~\cite{PhysRevD.83.074010} and pileup, and this method of approximate,
parametrized modelling does not easily scale to include such extra, complicated
effects.
For this special case it also cost a few second of computing time per event,
which is not competitive with modern Machine Learning algorithms that, once
trained, can be rapidly evaluated over many events and should approximate
the same result.
Although this study shows that we \emph{can} use theoretical modelling
with theoretically motivated algorithms for practical classification, it does
not show that we \emph{should}.
Empirically motivated function approximators such as
Boosted Decision Trees~\cite{xgboost} or
Neural Networks~\cite{MurphyKevinP.2012Mlap} are more scalable to complicated
uses and more computationally efficient in evaluation.


infamously arbitrary and subjective. This is reality. They are.
Consider this simple mathematical example with two propositions
$a=\textrm{``}x\textrm{ is even''}$ and
$b=\textrm{``}x\textrm{ equals }2\textrm{''}$.
Clearly $\prob{a}{b} = 1$ since $2$ is indeed even.
But what is $\prob{b}{a}$? We don't know. Perhaps one could argue symmetry
across all even numbers, but there are infinitely many of them so that
probability distribution could not be normalized.

\TODO{\ldots}

This is summarized most succinctly by Jaynes:
``any function with a single rounded maximum, raised to a higher and higher
power, goes into a Gaussian function''~\cite{jaynes2003probability}.


More important than this Wald's mathematical proof is that this coincides with
intuition --- when considering decisions, it feels normal to imagine what might
occur in each case and consider how good or bad the plausble outcomes are.

SMRW scan url http://psychclassics.yorku.ca/Fisher/Methods/


% Read2002cls
% }
\TODO{examples of use and misuse in hep}

\TODO{p-value footnote:
The chance of a small \pvalue\ is small.
This is true, but not interesting because the value of any \pvalue\ is small.
By construction \pvalues\ are uniformly distributed on the unit interval
under a given hypothesis.
The frequency of $p < 0.05$ is $5\%$, but so also is the frequency of
$0.6 < p < 0.65$.
The frequency of $p < \varepsilon$ is identical to the frequency
of $0.6 < p < 0.6 + \varepsilon$.
An appropriate choice of test statistic is therefore necessary for the
\pvalue\ to have meaning.
}

, and a small observed \pvalue\ does implies neither
a preference for the signal hypothesis nor incompatibility with the background.

% removed comment on jets vs muons --- think I discuss this elsewhere

%% Jet / muon start of discussion sand
% Jets are `messy', right?
% Jets do tend to worse resolution --- they suffer from calorimeter noise
% (which varies with statistical properties of hadronic showers
% along with hardware effects), as well as variable
% Jets include neutral hadrons which cannot be tracked, jet clustering may
% incorrectly assign objects, and some hadrons can decay to neutrinos and soft
% muons which would go undetected, and calorimetry is inherently variable.
% This tendency, however, fails for very hard muons.


\begin{equation}
\label{eqn:searches_logistic}
\prob{S}{d} = \frac{1}{1 + e^{-\Delta(d)}}
.
\end{equation}
Physically, this is analogous to the Boltzmann distribution where the energy
of each model is its negative $\log$ probability, with unit inverse
temperature~\cite{
skilling2017david,
lecun-06,
pmlr-v2-ranzato07a
}.
With multiple models in play, it is also called the `softmax'
function.


% whooo removing a section


\subsection{Gaussian significance}
Gaussian standard-deviations distance is an intuitive measure of compatibility
with a prediction.
than
a \pvalue.

defining $\chi = (x - \mu) / \sigma$, the Gaussian likelihood is

\begin{equation}
L(\chi) =
\frac{1}{\sqrt{2\pi}}
\exp\!\left(-\frac{1}{2}\chi^2\right)
\end{equation}


\TODO{log maximum likelihood ratio}
$\log$ maximum likelihood ratio $\kappa$
\begin{equation}
\kappa = -\frac{1}{2}\chi^2
\quad
\Leftrightarrow
\quad
\chi = \pm\sqrt{-2\kappa}
\end{equation}

\TODO{tail area}


\begin{align}
P(\chi)
&=
\frac{1}{\sqrt{2\pi}}
\int_\chi^\infty
\!
\exp\!\left(-\frac{1}{2}t^2\right)
\,\mathrm{d}t
\\[0.5em]
&=
L(\chi)
\times
\left(
\frac{1}{\chi}
+
\mathcal{O}\!\left(\frac{1}{\chi^3}\right)
\right)
\end{align}
\TODO{significance measures}
~\cite{
jaynes2003probability,
abramowitz1965handbook
}

% significance measures
ATLAS' recommended significance measure~\cite{atlas_significance} is
\begin{align}
\label{eqn:significance_atlas}
S_\mathrm{\atlas}(n; \mu, \sigma) =~&
\sqrt{2} \times
\mathrm{sign}(n - \mu) \times
\\[0.2em] \nonumber
&
\sqrt{
n\log\left(\frac{n(\mu + \sigma^2)}{\mu^2 + n\sigma^2}\right)
- \frac{\mu^2}{\sigma^2}\log\left(
1 + \frac{\sigma^2(n - \mu)}{\mu(\mu + \sigma^2)}
\right)
}
,
\end{align}
taken in a limit where $0\log(0x) = 0$ for the $n=0$ case.


%
\footnote{%
This context of a fixed inequality is one condition where the
\pvalue\ ratio is not misleading when interpreted as a likelihood ratio.%
}


%
\footnote{%
A draft of~\cite{HIGG-2018-51} stated that a \pvalue\ determined
``the probability that the background-only hypothesis is compatible with the
observed data''.
That text was removed in response to my internal comments, but
``probability of compatibility'' remains in its figure captions.%
}%
\footnote{%
Reference~\cite{HIGG-2018-57} accurately describes a \pvalue\ as a tail
probability, but also (repeatedly) states that a
``probability of compatibility\ldots\ corresponds to a \pvalue''.%
}%

% awful awful tangled up day

\TODO{B-hadrons and LHCb?}
% more cost-effective to improve calorimetry at the expense of PID

% measurements are supported by complete and precise calorimetry
% PID is expensive and less useful in comparison

To precise measure of these massive objects
justifies a focus on complete and precise calorimetry
at the expense of exact particle identification.

\TODO{
Argument:
want to measure W, t,
so measure jets and precise objects
so weaker B and heavy ion programme.
...
Refer to competitive measurements of W and t masses
and xsec of various rare processes (diboson, triboson)
}


Towards maximizing the quality of these precision measurements,


sensitivity to
exact hadron identities is comparatively less cost-effective.%


To maximize the quality of measurements like $m(t)$ and $m(W)$, it therefore
makes sense for \atlas' design to prefer complete and accurate calorimetry over
hadron identification.

Such hadronic momentum measurements contribute , both of which are
leveraged for \atlas' measurements of $m(t)$ and $m(W)$~\cite{
atlas2022symmarytop,
atlas2019topmass
}.

Although \atlas\ has an active programmes in other Standard Model research such
as B-physics and heavy-ion collisions, these are rarely competitive with the specialized experiments
\lhcb\ and ALICE respectively.
