As $\sigma \rightarrow 0$, this tends towards the the significance-transformed,
sign-flipped Poisson maximum likelihood ratio
\begin{equation}
S_\mathrm{Poisson}(n, \mu, \sigma)
= \mathrm{sgn}(n - \mu) \sqrt{2\left(
    n\log\left(\frac{n}{\mu}\right) - n + \mu
\right)}
~.
\end{equation}



\subsection{Higgs to tau-tau Nested Sampling}
Proof-of-concept implementation of the Nested Sampling algorithm~\cite{
skilling2004nested,
skilling2006nested,
skilling2010foundations
}
to classify events mediated by Higgs or Z bosons in their leptonic decays
through tau lepton pairs in \atlas.
In this scheme, Nested Sampling approximates likelihoods at given data for
approximate models of the Higgs and Z models by `marginalizing',
or integrating over, all freedoms in their production in LHC proton-proton
collisions.
Models of their processes were constructed from theory for the
helicity-dependent leptonic decays of tau leptons~\cite{Hays2014tau}
(with their two neutrinos integrated out analytically)
and Z bosons~\cite{Thomson2011EWK},
and parametrized from numerical parton distribution
functions~\cite{Buckley:2014ana} for the models' differing distributions of
longitudinal boosts.

Unlike the similar Matrix Element Method~\cite{
Fiedler:2010sg,
Gainer:2013iya,
PhysRevD.83.074010
},
which estimates likelihoods by fixing certain observed quantities and
integrating remaining freedoms with adaptive Monte Carlo algorithms~\cite{
Fiedler:2010sg,
Gainer:2013iya,
Lepage:1977sw,
Ohl:1998jn
},
with Nested Sampling we begin in ignorance of the data in each event and
gradually explore its constraints with random walks.
Euclidean coordinates projected to the surface of the unit ball,
in each of the respective rest frames of the heavy boson and tau leptons, allow
those walks to avoid the gimble lock of spherical polar coordinates, and are
used to describe Lorentz transformations to transform the observable leptons
to the lab frame.

This method worked in testing on smeared parton-level simulations from
\madgraph~\cite{Alwall:2014hca} and was presented in two conference posters.
\TODO{Should these posters be included as figures? Maybe appendix}
However, real events have substantial additional noise from QCD
radiation~\cite{PhysRevD.83.074010} and pileup, and this method of approximate,
parametrized modelling does not easily scale to include such extra, complicated
effects.
For this special case it also cost a few second of computing time per event,
which is not competitive with modern Machine Learning algorithms that, once
trained, can be rapidly evaluated over many events and should approximate
the same result.
Although this study shows that we \emph{can} use theoretical modelling
with theoretically motivated algorithms for practical classification, it does
not show that we \emph{should}.
Empirically motivated function approximators such as
Boosted Decision Trees~\cite{xgboost} or
Neural Networks~\cite{MurphyKevinP.2012Mlap} are more scalable to complicated
uses and more computationally efficient in evaluation.


infamously arbitrary and subjective. This is reality. They are.
Consider this simple mathematical example with two propositions
$a=\textrm{``}x\textrm{ is even''}$ and
$b=\textrm{``}x\textrm{ equals }2\textrm{''}$.
Clearly $\prob{a}{b} = 1$ since $2$ is indeed even.
But what is $\prob{b}{a}$? We don't know. Perhaps one could argue symmetry
across all even numbers, but there are infinitely many of them so that
probability distribution could not be normalized.

\TODO{\ldots}

This is summarized most succinctly by Jaynes:
``any function with a single rounded maximum, raised to a higher and higher
power, goes into a Gaussian function''~\cite{jaynes2003probability}.


More important than this Wald's mathematical proof is that this coincides with
intuition --- when considering decisions, it feels normal to imagine what might
occur in each case and consider how good or bad the plausble outcomes are.

SMRW scan url http://psychclassics.yorku.ca/Fisher/Methods/


% Read2002cls
% }
\TODO{examples of use and misuse in hep}

\TODO{p-value footnote:
The chance of a small \pvalue\ is small.
This is true, but not interesting because the value of any \pvalue\ is small.
By construction \pvalues\ are uniformly distributed on the unit interval
under a given hypothesis.
The frequency of $p < 0.05$ is $5\%$, but so also is the frequency of
$0.6 < p < 0.65$.
The frequency of $p < \varepsilon$ is identical to the frequency
of $0.6 < p < 0.6 + \varepsilon$.
An appropriate choice of test statistic is therefore necessary for the
\pvalue\ to have meaning.
}

, and a small observed \pvalue\ does implies neither
a preference for the signal hypothesis nor incompatibility with the background.

% removed comment on jets vs muons --- think I discuss this elsewhere

%% Jet / muon start of discussion sand
% Jets are `messy', right?
% Jets do tend to worse resolution --- they suffer from calorimeter noise
% (which varies with statistical properties of hadronic showers
% along with hardware effects), as well as variable
% Jets include neutral hadrons which cannot be tracked, jet clustering may
% incorrectly assign objects, and some hadrons can decay to neutrinos and soft
% muons which would go undetected, and calorimetry is inherently variable.
% This tendency, however, fails for very hard muons.


\begin{equation}
\label{eqn:searches_logistic}
\prob{S}{d} = \frac{1}{1 + e^{-\Delta(d)}}
.
\end{equation}
Physically, this is analogous to the Boltzmann distribution where the energy
of each model is its negative $\log$ probability, with unit inverse
temperature~\cite{
skilling2017david,
lecun-06,
pmlr-v2-ranzato07a
}.
With multiple models in play, it is also called the `softmax'
function.


% whooo removing a section


\subsection{Gaussian significance}
Gaussian standard-deviations distance is an intuitive measure of compatibility
with a prediction.
than
a \pvalue.

defining $\chi = (x - \mu) / \sigma$, the Gaussian likelihood is

\begin{equation}
L(\chi) =
\frac{1}{\sqrt{2\pi}}
\exp\!\left(-\frac{1}{2}\chi^2\right)
\end{equation}


\TODO{log maximum likelihood ratio}
$\log$ maximum likelihood ratio $\kappa$
\begin{equation}
\kappa = -\frac{1}{2}\chi^2
\quad
\Leftrightarrow
\quad
\chi = \pm\sqrt{-2\kappa}
\end{equation}

\TODO{tail area}


\begin{align}
P(\chi)
&=
\frac{1}{\sqrt{2\pi}}
\int_\chi^\infty
\!
\exp\!\left(-\frac{1}{2}t^2\right)
\,\mathrm{d}t
\\[0.5em]
&=
L(\chi)
\times
\left(
\frac{1}{\chi}
+
\mathcal{O}\!\left(\frac{1}{\chi^3}\right)
\right)
\end{align}
\TODO{significance measures}
~\cite{
jaynes2003probability,
abramowitz1965handbook
}

% significance measures
ATLAS's recommended significance measure~\cite{atlas_significance} is
\begin{align}
\label{eqn:significance_atlas}
S_\mathrm{\atlas}(n; \mu, \sigma) =~&
\sqrt{2} \times
\mathrm{sign}(n - \mu) \times
\\[0.2em] \nonumber
&
\sqrt{
n\log\left(\frac{n(\mu + \sigma^2)}{\mu^2 + n\sigma^2}\right)
- \frac{\mu^2}{\sigma^2}\log\left(
1 + \frac{\sigma^2(n - \mu)}{\mu(\mu + \sigma^2)}
\right)
}
,
\end{align}
taken in a limit where $0\log(0x) = 0$ for the $n=0$ case.


%
\footnote{%
This context of a fixed inequality is one condition where the
\pvalue\ ratio is not misleading when interpreted as a likelihood ratio.%
}


%
\footnote{%
A draft of~\cite{HIGG-2018-51} stated that a \pvalue\ determined
``the probability that the background-only hypothesis is compatible with the
observed data''.
That text was removed in response to my internal comments, but
``probability of compatibility'' remains in its figure captions.%
}%
\footnote{%
Reference~\cite{HIGG-2018-57} accurately describes a \pvalue\ as a tail
probability, but also (repeatedly) states that a
``probability of compatibility\ldots\ corresponds to a \pvalue''.%
}%

% awful awful tangled up day

\TODO{B-hadrons and LHCb?}
% more cost-effective to improve calorimetry at the expense of PID

% measurements are supported by complete and precise calorimetry
% PID is expensive and less useful in comparison

To precise measure of these massive objects
justifies a focus on complete and precise calorimetry
at the expense of exact particle identification.

\TODO{
Argument:
want to measure W, t,
so measure jets and precise objects
so weaker B and heavy ion programme.
...
Refer to competitive measurements of W and t masses
and xsec of various rare processes (diboson, triboson)
}


Towards maximizing the quality of these precision measurements,


sensitivity to
exact hadron identities is comparatively less cost-effective.%


To maximize the quality of measurements like $m(t)$ and $m(W)$, it therefore
makes sense for \atlas' design to prefer complete and accurate calorimetry over
hadron identification.

Such hadronic momentum measurements contribute , both of which are
leveraged for \atlas' measurements of $m(t)$ and $m(W)$~\cite{
atlas2022symmarytop,
atlas2019topmass
}.

Although \atlas\ has an active programmes in other Standard Model research such
as B-physics and heavy-ion collisions, these are rarely competitive with the specialized experiments
\lhcb\ and ALICE respectively.




(contributing to the recent storm around a conflicting
result~\cite{cdf2022high})


, notably with a special use of
tungsten absorbers in the forward hadronic calorimeters.


% prob cutting

Two algebraic rules derive from Equation~\ref{eqn:searches_prob_ratio}:
a \textbf{product rule}
\begin{align}
\prob{c, b}{a} &= \prob{c}{b, a}\times \prob{b}{a},
\label{eqn:searches_product_rule}
\intertext{%
describing nested proportions
(a spoon in a teacup in a sink),
and a \textbf{sum rule}%
}
\prob{c\;\textrm{or}\;b}{a} &= \prob{c}{a} + \prob{b}{a} - \prob{c, b}{a},
\label{eqn:searches_sum_rule}
\end{align}
describing combination in a union;
the subtraction avoids double-counting of any overlap and is usually avoided by
splitting any problem into disjoint, orthogonal states.
For disjoint cases with labels $b_i$, normalization requires that
\begin{equation}
\label{eqn:searches_normalization}
\sum_i \prob{b_i}{a} = 1.
\end{equation}
These basic rules are well known, but we state them here to support the
following statements on how probabilities can applied for practical results.%
\footnote{%
The formulation with sums and products is not strictly unique, but all
alternatives are equivalent up to an invertible
transformation~\cite{axioms1010038}.
This choice is often best because it uses ordinary addition and
multiplication, but $\log$ probabilities can avoid overflows outside the ranges
representable by floating-point numbers in numerical analysis,
thus changing multiplication to addition and addition to the
`$\log$-add-$\exp$' operation.%
}


\\[.5em]
Particle lifetimes follow exponential distributions, and their maximum
densities are at zero time, but it is incorrect to describe their most probable
lifetimes as zero.
the `most probable' $\log$ lifetime is not $\log 0$, but turns out to coincide
with the mean lifetime.
\\[.5em]
For zero Poisson data, the maximum likelihood rate is zero, but zero might not
be the most probable rate (if, for example, an experiment has known backgrounds).


% excess philosophy
Theory digests data to extract useful information.
Those data include not only observations from scientific experiments,
but the humanity's accumulated knowledge that one might glean from books,
intuitions that one learns by exploring the world,
and even fragments inherited in biology.
The nutrients that theoretical work extracts are deeper concepts,
such as postulates or mathematical models,
which might reconstruct implications of the data and make useful or novel
predictions.

The current Standard Model of particle physics is a magnificent results from
theoretical and experimental works which, over the last century or
so, collaborated to understand and observe the actions of nature's most
fundamental known particles.
It now comprises a solid mathematical theory that accurately describes the
physics of particle interactions across a broad domain.
But it is not an ultimate theory of everything; that domain is bounded, and
work continues to expand those boundaries.

To support the later work that this thesis presents on experimental data from
the \atlas\ detector, this chapter introduces contextual information.
 presents the Standard Model of particle
physics, both in its current state and possible future directions.
Section~\ref{sec:context_atlas} then describes the \atlas\ detector.
\TODO{more}


These are classified into the strong sector
% quarks
\begin{itemize}
\item up-type: up, charm, top ($u$, $c$, $t$), and
\item down-type: down, strange, bottom ($d$, $s$, $b$),
\end{itemize}
% leptons

% ex quotes

\begin{singlespacing}
\section{The Standard Model and beyond}
\label{sec:context_sm_and_beyond}
%
\begin{epigraphs}
\qitem{%
In fact, it is a defensible conjecture that no
theory can be complete and inviolable that emerges from finitely equipped minds
and survives finitely scoped experiment, which are the twin rickety foundations
for all theories.%
}%
{James~D.~Wells,
\textit{Discovery Beyond the Standard Model of~Elementary~Particle~Physics},
2019~\cite{wells2020discovery}}
\end{epigraphs}
\end{singlespacing}


\begin{singlespacing}
\section{The \atlas\ Experiment}
\label{sec:context_atlas}
%
\begin{epigraphs}
\qitem{%
if observing outer space gives us a view of the past, observing inner space
would surely give us a glimpse into the future - would be interesting if NASA
made a telescope for that%
}%
{Ken~M,
\textit{Comment: Yahoo! News},
2012~\cite{kenm2012inner}}
\end{epigraphs}
\end{singlespacing}


% \qitem{%
% One geometry can not be more true than another; it can only be more convenient.
% Geometry is not true, it is advantageous.}%
% {Robert~M.~Pirsig,
% \textit{Zen and the Art of Motorcycle Maintenance},
% 1974~\cite{pirsig1999zen}}



So are Newton's laws in their domain; both models are useful in different
applications.



Supersymmetry itself is not a model, but if our searches did hypothetically
find evidence for any supersymmetric model, then that would be evidence for
supersymmetry in nature.
