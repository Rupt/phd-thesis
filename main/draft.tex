As $\sigma \rightarrow 0$, this tends towards the the significance-transformed,
sign-flipped Poisson maximum likelihood ratio
\begin{equation}
S_\mathrm{Poisson}(n, \mu, \sigma)
= \mathrm{sgn}(n - \mu) \sqrt{2\left(
    n\log\left(\frac{n}{\mu}\right) - n + \mu
\right)}
~.
\end{equation}



\subsection{Higgs to tau-tau Nested Sampling}
Proof-of-concept implementation of the Nested Sampling algorithm~\cite{
skilling2004nested,
skilling2006nested,
skilling2010foundations
}
to classify events mediated by Higgs or Z bosons in their leptonic decays
through tau lepton pairs in \atlas.
In this scheme, Nested Sampling approximates likelihoods at given data for
approximate models of the Higgs and Z models by `marginalizing',
or integrating over, all freedoms in their production in LHC proton-proton
collisions.
Models of their processes were constructed from theory for the
helicity-dependent leptonic decays of tau leptons~\cite{Hays2014tau}
(with their two neutrinos integrated out analytically)
and Z bosons~\cite{Thomson2011EWK},
and parametrized from numerical parton distribution
functions~\cite{Buckley:2014ana} for the models' differing distributions of
longitudinal boosts.

Unlike the similar Matrix Element Method~\cite{
Fiedler:2010sg,
Gainer:2013iya,
PhysRevD.83.074010
},
which estimates likelihoods by fixing certain observed quantities and
integrating remaining freedoms with adaptive Monte Carlo algorithms~\cite{
Fiedler:2010sg,
Gainer:2013iya,
Lepage:1977sw,
Ohl:1998jn
},
with Nested Sampling we begin in ignorance of the data in each event and
gradually explore its constraints with random walks.
Euclidean coordinates projected to the surface of the unit ball,
in each of the respective rest frames of the heavy boson and tau leptons, allow
those walks to avoid the gimble lock of spherical polar coordinates, and are
used to describe Lorentz transformations to transform the observable leptons
to the lab frame.

This method worked in testing on smeared parton-level simulations from
\madgraph~\cite{Alwall:2014hca} and was presented in two conference posters.
\TODO{Should these posters be included as figures? Maybe appendix}
However, real events have substantial additional noise from QCD
radiation~\cite{PhysRevD.83.074010} and pileup, and this method of approximate,
parametrized modelling does not easily scale to include such extra, complicated
effects.
For this special case it also cost a few second of computing time per event,
which is not competitive with modern Machine Learning algorithms that, once
trained, can be rapidly evaluated over many events and should approximate
the same result.
Although this study shows that we \emph{can} use theoretical modelling
with theoretically motivated algorithms for practical classification, it does
not show that we \emph{should}.
Empirically motivated function approximators such as
Boosted Decision Trees~\cite{xgboost} or
Neural Networks~\cite{MurphyKevinP.2012Mlap} are more scalable to complicated
uses and more computationally efficient in evaluation.


infamously arbitrary and subjective. This is reality. They are.
Consider this simple mathematical example with two propositions
$a=\textrm{``}x\textrm{ is even''}$ and
$b=\textrm{``}x\textrm{ equals }2\textrm{''}$.
Clearly $\prob{a}{b} = 1$ since $2$ is indeed even.
But what is $\prob{b}{a}$? We don't know. Perhaps one could argue symmetry
across all even numbers, but there are infinitely many of them so that
probability distribution could not be normalized.

\TODO{\ldots}

This is summarized most succinctly by Jaynes:
``any function with a single rounded maximum, raised to a higher and higher
power, goes into a Gaussian function''~\cite{jaynes2003probability}.


More important than this Wald's mathematical proof is that this coincides with
intuition --- when considering decisions, it feels normal to imagine what might
occur in each case and consider how good or bad the plausble outcomes are.

SMRW scan url http://psychclassics.yorku.ca/Fisher/Methods/


% Read2002cls
% }
\TODO{examples of use and misuse in hep}

\TODO{p-value footnote:
The chance of a small \pvalue\ is small.
This is true, but not interesting because the value of any \pvalue\ is small.
By construction \pvalues\ are uniformly distributed on the unit interval
under a given hypothesis.
The frequency of $p < 0.05$ is $5\%$, but so also is the frequency of
$0.6 < p < 0.65$.
The frequency of $p < \varepsilon$ is identical to the frequency
of $0.6 < p < 0.6 + \varepsilon$.
An appropriate choice of test statistic is therefore necessary for the
\pvalue\ to have meaning.
}

, and a small observed \pvalue\ does implies neither
a preference for the signal hypothesis nor incompatibility with the background.

% removed comment on jets vs muons --- think I discuss this elsewhere

%% Jet / muon start of discussion sand
% Jets are `messy', right?
% Jets do tend to worse resolution --- they suffer from calorimeter noise
% (which varies with statistical properties of hadronic showers
% along with hardware effects), as well as variable
% Jets include neutral hadrons which cannot be tracked, jet clustering may
% incorrectly assign objects, and some hadrons can decay to neutrinos and soft
% muons which would go undetected, and calorimetry is inherently variable.
% This tendency, however, fails for very hard muons.


\begin{equation}
\label{eqn:searches_logistic}
\prob{S}{d} = \frac{1}{1 + e^{-\Delta(d)}}
.
\end{equation}
Physically, this is analogous to the Boltzmann distribution where the energy
of each model is its negative $\log$ probability, with unit inverse
temperature~\cite{
skilling2017david,
lecun-06,
pmlr-v2-ranzato07a
}.
With multiple models in play, it is also called the `softmax'
function.


% whooo removing a section


\subsection{Gaussian significance}
Gaussian standard-deviations distance is an intuitive measure of compatibility
with a prediction.
than
a \pvalue.

defining $\chi = (x - \mu) / \sigma$, the Gaussian likelihood is

\begin{equation}
L(\chi) =
\frac{1}{\sqrt{2\pi}}
\exp\!\left(-\frac{1}{2}\chi^2\right)
\end{equation}


\TODO{log maximum likelihood ratio}
$\log$ maximum likelihood ratio $\kappa$
\begin{equation}
\kappa = -\frac{1}{2}\chi^2
\quad
\Leftrightarrow
\quad
\chi = \pm\sqrt{-2\kappa}
\end{equation}

\TODO{tail area}


\begin{align}
P(\chi)
&=
\frac{1}{\sqrt{2\pi}}
\int_\chi^\infty
\!
\exp\!\left(-\frac{1}{2}t^2\right)
\,\mathrm{d}t
\\[0.5em]
&=
L(\chi)
\times
\left(
\frac{1}{\chi}
+
\mathcal{O}\!\left(\frac{1}{\chi^3}\right)
\right)
\end{align}
\TODO{significance measures}
~\cite{
jaynes2003probability,
abramowitz1965handbook
}

% significance measures
ATLAS's recommended significance measure~\cite{atlas_significance} is
\begin{align}
\label{eqn:significance_atlas}
S_\mathrm{\atlas}(n; \mu, \sigma) =~&
\sqrt{2} \times
\mathrm{sign}(n - \mu) \times
\\[0.2em] \nonumber
&
\sqrt{
n\log\left(\frac{n(\mu + \sigma^2)}{\mu^2 + n\sigma^2}\right)
- \frac{\mu^2}{\sigma^2}\log\left(
1 + \frac{\sigma^2(n - \mu)}{\mu(\mu + \sigma^2)}
\right)
}
,
\end{align}
taken in a limit where $0\log(0x) = 0$ for the $n=0$ case.


%
\footnote{%
This context of a fixed inequality is one condition where the
\pvalue\ ratio is not misleading when interpreted as a likelihood ratio.%
}


%
\footnote{%
A draft of~\cite{HIGG-2018-51} stated that a \pvalue\ determined
``the probability that the background-only hypothesis is compatible with the
observed data''.
That text was removed in response to my internal comments, but
``probability of compatibility'' remains in its figure captions.%
}%
\footnote{%
Reference~\cite{HIGG-2018-57} accurately describes a \pvalue\ as a tail
probability, but also (repeatedly) states that a
``probability of compatibility\ldots\ corresponds to a \pvalue''.%
}%

% awful awful tangled up day

\TODO{B-hadrons and LHCb?}
% more cost-effective to improve calorimetry at the expense of PID

% measurements are supported by complete and precise calorimetry
% PID is expensive and less useful in comparison

To precise measure of these massive objects
justifies a focus on complete and precise calorimetry
at the expense of exact particle identification.

\TODO{
Argument:
want to measure W, t,
so measure jets and precise objects
so weaker B and heavy ion programme.
...
Refer to competitive measurements of W and t masses
and xsec of various rare processes (diboson, triboson)
}


Towards maximizing the quality of these precision measurements,


sensitivity to
exact hadron identities is comparatively less cost-effective.%


To maximize the quality of measurements like $m(t)$ and $m(W)$, it therefore
makes sense for \atlas' design to prefer complete and accurate calorimetry over
hadron identification.

Such hadronic momentum measurements contribute , both of which are
leveraged for \atlas' measurements of $m(t)$ and $m(W)$~\cite{
atlas2022symmarytop,
atlas2019topmass
}.

Although \atlas\ has an active programmes in other Standard Model research such
as B-physics and heavy-ion collisions, these are rarely competitive with the specialized experiments
\lhcb\ and ALICE respectively.




(contributing to the recent storm around a conflicting
result~\cite{cdf2022high})


, notably with a special use of
tungsten absorbers in the forward hadronic calorimeters.


% prob cutting

Two algebraic rules derive from Equation~\ref{eqn:searches_prob_ratio}:
a \textbf{product rule}
\begin{align}
\prob{c, b}{a} &= \prob{c}{b, a}\times \prob{b}{a},
\label{eqn:searches_product_rule}
\intertext{%
describing nested proportions
(a spoon in a teacup in a sink),
and a \textbf{sum rule}%
}
\prob{c\;\textrm{or}\;b}{a} &= \prob{c}{a} + \prob{b}{a} - \prob{c, b}{a},
\label{eqn:searches_sum_rule}
\end{align}
describing combination in a union;
the subtraction avoids double-counting of any overlap and is usually avoided by
splitting any problem into disjoint, orthogonal states.
For disjoint cases with labels $b_i$, normalization requires that
\begin{equation}
\label{eqn:searches_normalization}
\sum_i \prob{b_i}{a} = 1.
\end{equation}
These basic rules are well known, but we state them here to support the
following statements on how probabilities can applied for practical results.%
\footnote{%
The formulation with sums and products is not strictly unique, but all
alternatives are equivalent up to an invertible
transformation~\cite{axioms1010038}.
This choice is often best because it uses ordinary addition and
multiplication, but $\log$ probabilities can avoid overflows outside the ranges
representable by floating-point numbers in numerical analysis,
thus changing multiplication to addition and addition to the
`$\log$-add-$\exp$' operation.%
}


\\[.5em]
Particle lifetimes follow exponential distributions, and their maximum
densities are at zero time, but it is incorrect to describe their most probable
lifetimes as zero.
the `most probable' $\log$ lifetime is not $\log 0$, but turns out to coincide
with the mean lifetime.
\\[.5em]
For zero Poisson data, the maximum likelihood rate is zero, but zero might not
be the most probable rate (if, for example, an experiment has known backgrounds).


% excess philosophy
Theory digests data to extract useful information.
Those data include not only observations from scientific experiments,
but the humanity's accumulated knowledge that one might glean from books,
intuitions that one learns by exploring the world,
and even fragments inherited in biology.
The nutrients that theoretical work extracts are deeper concepts,
such as postulates or mathematical models,
which might reconstruct implications of the data and make useful or novel
predictions.

The current Standard Model of particle physics is a magnificent results from
theoretical and experimental works which, over the last century or
so, collaborated to understand and observe the actions of nature's most
fundamental known particles.
It now comprises a solid mathematical theory that accurately describes the
physics of particle interactions across a broad domain.
But it is not an ultimate theory of everything; that domain is bounded, and
work continues to expand those boundaries.

To support the later work that this thesis presents on experimental data from
the \atlas\ detector, this chapter introduces contextual information.
 presents the Standard Model of particle
physics, both in its current state and possible future directions.
Section~\ref{sec:context_atlas} then describes the \atlas\ detector.
\TODO{more}


These are classified into the strong sector
% quarks
\begin{itemize}
\item up-type: up, charm, top ($u$, $c$, $t$), and
\item down-type: down, strange, bottom ($d$, $s$, $b$),
\end{itemize}
% leptons

% ex quotes

\begin{singlespacing}
\section{The Standard Model and beyond}
\label{sec:context_sm_and_beyond}
%
\begin{epigraphs}
\qitem{%
In fact, it is a defensible conjecture that no
theory can be complete and inviolable that emerges from finitely equipped minds
and survives finitely scoped experiment, which are the twin rickety foundations
for all theories.%
}%
{James~D.~Wells,
\textit{Discovery Beyond the Standard Model of~Elementary~Particle~Physics},
2019~\cite{wells2020discovery}}
\end{epigraphs}
\end{singlespacing}


\begin{singlespacing}
\section{The \atlas\ Experiment}
\label{sec:context_atlas}
%
\begin{epigraphs}
\qitem{%
if observing outer space gives us a view of the past, observing inner space
would surely give us a glimpse into the future - would be interesting if NASA
made a telescope for that%
}%
{Ken~M,
\textit{Comment: Yahoo! News},
2012~\cite{kenm2012inner}}
\end{epigraphs}
\end{singlespacing}


% \qitem{%
% One geometry can not be more true than another; it can only be more convenient.
% Geometry is not true, it is advantageous.}%
% {Robert~M.~Pirsig,
% \textit{Zen and the Art of Motorcycle Maintenance},
% 1974~\cite{pirsig1999zen}}



So are Newton's laws in their domain; both models are useful in different
applications.



Supersymmetry itself is not a model, but if our searches did hypothetically
find evidence for any supersymmetric model, then that would be evidence for
supersymmetry in nature.


% for preamble

\noindent Theory is needed to understand experimental data,
and data are needed to improve theoretical models.
To clarify the experimental work of this thesis, this chapter introduces its
theoretical and experimental contexts;
Section~\ref{sec:context_sm_and_beyond} presents the current culmination of
theoretical particle physics in the Standard Model, its limitations, and its
possible future directions including supersymmetric ideas, and
Section~\ref{sec:context_atlas} then describes the \atlas\ detector, with whose
data we hope refine the Standard Model into its improved future forms.



% far too much intro waffle

\TODO{This does not belong in an introduction and will be moved.}

\TODO{CUT THIS SHORT}

\subsection{Testing symmetries in data}
Data-driven tests of physical symmetries are a speciality of the Cambridge
\atlas\ group.
Each test considers a theoretically motivated symmetry that states that
distributions of data should be identical when transformed under its operation.
This means that it can be possible to disprove a symmetry by searching only
for differences between data and transformed versions of those data, without
necessarily requiring other theoretical inputs.

% emus
Charge symmetry of electron-muon, `emu', pairs is a key example among these
tests.
Theoretically, it proposes that, in the absence of biases, data
from proton-proton collisions at the LHC would include equal distributions
of $e^+\mu^-$ and $e^-\mu^+$ events~\cite{Lester:2016qdv}.
Real data can have emu biases --- one clear (but minor) example is that our
detectors are made of matter containing negatively charged electrons, not
positrons, so high energy electrons positrons have subtly different rates of
energy loss in that detector matter.
However, \cite{Lester:2016qdv} argues that any biases at the LHC should
conspire only to favour the rate of $e^-\mu^+$ over the rate of $e^+\mu^-$
events.
Therefore defining an asymmetry
\begin{equation}
A^\mathrm{emu} = \frac{
\sigma(e^+\mu^- + X)
}{
\sigma(e^-\mu^+ + X)
}
,
\end{equation}
one can predict $A^\mathrm{emu} \leq 1$ from the Standard Model at the LHC.
Negative values could be explained by experimental biases, but
$A^\mathrm{emu} > 1$ measured in data might indicate new physics, since it and
can be caused by models of new physics that might invoke R-parity violating
supersymmetry~\cite{Lester:2016qdv} or leptoquarks~\cite{EXOT-2018-29}.
After its extensive development~\cite{Brunt:2674708,Pacey:2747774},
this test has been performed in \atlas\ data, and found no ``significant
model-independent evidence''~\cite{EXOT-2018-29} for $A^\mathrm{emu} > 1$.

% parity violation
Parity symmetry, which changes the signs of all spatial coordinates
$\vec x\rightarrow -\vec x$, is a second key example.
Nature does violate parity symmetry, as first shown by the Wu
experiment~\cite{PhysRev.105.1413}, in the weak sector, but that is not
directly observable at the LHC due to its non-polarized beams and
polarization-insensitive detectors~\cite{Lester:2019bso}.
Unknown physical effects could, however, generate forms of parity asymmetry
that could be seen at the LHC.
Although the charge symmetry of the emu test has known sources of bias,
no such biases are so clearly expected for parity.

General parity symmetry can therefore be tested at the LHC by considering a
parity-odd event variable and comparing its distribution to its parity-flipped
version, which simply swaps its sign.
% alpha from lester schott
One such test has been performed with LHC data~\cite{Lester:2019bso}.
This test used \cms\ open data and the parity-odd event variable
\begin{equation}
\label{eqn:cms_alpha}
\alpha
= \arcsin\!
\left(
\frac{
\vec p^{\,j_1} \times \vec p^{\,j_2}
}{
|\vec p^{\,j_1} \times \vec p^{\,j_2}|
}
\cdot
\frac{
\vec p^{\,j_3}
}{
|\vec p^{\,j_3}|
}
\right)
,
\end{equation}
in which $p^{\,j_i}$ is the momentum of the $i$th hardest jet in a multi-jet
event,
and did not find evidence for parity violation.
But that does not prove that no parity-violating effects are occuring in these
data, only that it did not appear in the distribution of $\alpha$.

Can a single parity-odd event variable be sensitive to all norms of parity
violation?
The answer, it turns out, is `no' if that variable is also required to be
continuous~\cite{lesterChiralMeasurements2021}.
In this framework, therefore, a joint analysis of multiple variables is
required for any sufficiently complicated data.
Furthermore, it is not generally easy to construct such universally-sensitive
sets of event variables~\cite{
Gripaios:2020hya,
lester2021lorentz
}.

Rather than beginning from universally sensitive event variables, an
alternative strategy is to build candidate variables to the data
themselves~\cite{lester2021stressed}.
If done wrong, such an idea could simply reinforce variations due to simple
statistical noise.
It can, however, be formulated as a rigorous test if one employs standard
Machine Learning practice and splits the data into independent training and
testing sets.
Variables learned (by function approximation algorithms such as
Boosted Decision Trees~\cite{xgboost} or
Neural Networks~\cite{MurphyKevinP.2012Mlap})
from the testing set can correspond to hypotheses about possible asymmetries
in the data distribution, and if those hypotheses make accurate predictions in
independent testing data, then they are evidence against the symmetry.

My contribution to this field is to adapt this idea of learning parity
variables,
\emph{which was initiated by Christopher~G.~Lester}~\cite{lester2021stressed},
into a probabilistic description, in which the algorithms predict
`which is real?' between real data an their parity-transformed
copies~\cite{tombs2021which}.
This approach derives an objective function
(which is a special case of the binary
cross-entropy~\cite{MurphyKevinP.2012Mlap}),
from which learning algorithms
can approximate the odds ratio between the data distribution and its
parity-transformed version.
Importantly, it also allows one to model inefficiencies or holes in the
distribution, which exist in practice from kinematic selections and imperfect
hardware, and incorporate those into the training and testing without biasing
the result.
We have also demonstrated this method on simulated multi-jet data in an
\atlas-like detector~\cite{lester2022hunting}.
By simulating parity-violating events from a Lorentz-violating extension
to the Standard Model, we show that this method build variables that are much
more sensitive than the $\alpha$ of Equation~\ref{eqn:cms_alpha} while
retaining its other attractive symmetries by encoding them into our function
architectures~\cite{lester2021stressed, tombs2021which}.

This work demonstrates that the `which is real?' method is ready to test for
parity violation in real LHC data.
Much like the emu work, however, its realization would require great diligence
to handle important considerations of systematic uncertainties and statistical
interpretations before it could be published through \atlas.

The `which is real?' method is general, and can test not only parity, but
symmetry under any transformation of the data, irrespective of whether it is
discrete or continuous.
In the context of modern machine learning, it is an example of
`self-supervised' learning, in which the training objective derives from the
data and not from additional labels~\cite{
Noroozi2016jigsaw,
multitaskself2017,
devlin2019bert
}, and quite similar to the `CLIP' objective, which also attempts to identify
a real examples from collections of alternatives~\cite{pmlr-v139-radford21a}.
What is special in our work is our application for scientific self-supervised
testing.



Phrased succinctly by Skilling~\cite{skilling2008rant},
``we SHOULD use Bayes by assigning subjective priors and inspecting evidence
values.''
This f


% Here, $\kappa$ and $\sigma$ both relate to the second derivative of the target
% function; this relationship extends to multiple dimensions in the Hessian
% matrix and its inverse, and permits the useful approximation of estimate error
% bars from second derivatives.

% Among many applications, learning algorithms have practical successes in
% probability assignments, such as to predict labels in classification,
% or to predict next words in text generation~\cite{
% MurphyKevinP.2012Mlap,
% radford2019language
% }.
which


When arguing that we should report likelihoods and evidence values for our
data, we have not explicitly constructed cost functions, nor have we made
quantitative predictions of future events.
But we can qualitatively consider them in this decision theory framework ---
we imagine that future scientists will want to reuse our data, and would be
pleased (attain a small cost) if they were able to reinterpret the results for
their own models.
We can therefore reason that likelihoods would usually have more utility than
alternatives such as point posterior averages.


% Would like something along the lines of:
% `No formal derivations except for historical precedent,
% so review with that history'
% But feels too rude.
