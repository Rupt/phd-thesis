As $\sigma \rightarrow 0$, this tends towards the the significance-transformed,
sign-flipped Poisson maximum likelihood ratio
\begin{equation}
S_\mathrm{Poisson}(n, \mu, \sigma)
= \mathrm{sgn}(n - \mu) \sqrt{2\left(
    n\log\left(\frac{n}{\mu}\right) - n + \mu
\right)}
~.
\end{equation}



\subsection{Higgs to tau-tau Nested Sampling}
Proof-of-concept implementation of the Nested Sampling algorithm~\cite{
skilling2004nested,
skilling2006nested,
skilling2010foundations
}
to classify events mediated by Higgs or Z bosons in their leptonic decays
through tau lepton pairs in \atlas.
In this scheme, Nested Sampling approximates likelihoods at given data for
approximate models of the Higgs and Z models by `marginalizing',
or integrating over, all freedoms in their production in LHC proton-proton
collisions.
Models of their processes were constructed from theory for the
helicity-dependent leptonic decays of tau leptons~\cite{Hays2014tau}
(with their two neutrinos integrated out analytically)
and Z bosons~\cite{Thomson2011EWK},
and parametrized from numerical parton distribution
functions~\cite{Buckley:2014ana} for the models' differing distributions of
longitudinal boosts.

Unlike the similar Matrix Element Method~\cite{
Fiedler:2010sg,
Gainer:2013iya,
PhysRevD.83.074010
},
which estimates likelihoods by fixing certain observed quantities and
integrating remaining freedoms with adaptive Monte Carlo algorithms~\cite{
Fiedler:2010sg,
Gainer:2013iya,
Lepage:1977sw,
Ohl:1998jn
},
with Nested Sampling we begin in ignorance of the data in each event and
gradually explore its constraints with random walks.
Euclidean coordinates projected to the surface of the unit ball,
in each of the respective rest frames of the heavy boson and tau leptons, allow
those walks to avoid the gimble lock of spherical polar coordinates, and are
used to describe Lorentz transformations to transform the observable leptons
to the lab frame.

This method worked in testing on smeared parton-level simulations from
\madgraph~\cite{Alwall:2014hca} and was presented in two conference posters.
\TODO{Should these posters be included as figures? Maybe appendix}
However, real events have substantial additional noise from QCD
radiation~\cite{PhysRevD.83.074010} and pileup, and this method of approximate,
parametrized modelling does not easily scale to include such extra, complicated
effects.
For this special case it also cost a few second of computing time per event,
which is not competitive with modern Machine Learning algorithms that, once
trained, can be rapidly evaluated over many events and should approximate
the same result.
Although this study shows that we \emph{can} use theoretical modelling
with theoretically motivated algorithms for practical classification, it does
not show that we \emph{should}.
Empirically motivated function approximators such as
Boosted Decision Trees~\cite{xgboost} or
Neural Networks~\cite{MurphyKevinP.2012Mlap} are more scalable to complicated
uses and more computationally efficient in evaluation.


infamously arbitrary and subjective. This is reality. They are.
Consider this simple mathematical example with two propositions
$a=\textrm{``}x\textrm{ is even''}$ and
$b=\textrm{``}x\textrm{ equals }2\textrm{''}$.
Clearly $\prob{a}{b} = 1$ since $2$ is indeed even.
But what is $\prob{b}{a}$? We don't know. Perhaps one could argue symmetry
across all even numbers, but there are infinitely many of them so that
probability distribution could not be normalized.

\TODO{\ldots}

This is summarized most succinctly by Jaynes:
``any function with a single rounded maximum, raised to a higher and higher
power, goes into a Gaussian function''~\cite{jaynes2003probability}.


More important than this Wald's mathematical proof is that this coincides with
intuition --- when considering decisions, it feels normal to imagine what might
occur in each case and consider how good or bad the plausble outcomes are.

SMRW scan url http://psychclassics.yorku.ca/Fisher/Methods/


% Read2002cls
% }
\TODO{examples of use and misuse in hep}

\TODO{p-value footnote:
The chance of a small \pvalue\ is small.
This is true, but not interesting because the value of any \pvalue\ is small.
By construction \pvalues\ are uniformly distributed on the unit interval
under a given hypothesis.
The frequency of $p < 0.05$ is $5\%$, but so also is the frequency of
$0.6 < p < 0.65$.
The frequency of $p < \varepsilon$ is identical to the frequency
of $0.6 < p < 0.6 + \varepsilon$.
An appropriate choice of test statistic is therefore necessary for the
\pvalue\ to have meaning.
}

, and a small observed \pvalue\ does implies neither
a preference for the signal hypothesis nor incompatibility with the background.

% removed comment on jets vs muons --- think I discuss this elsewhere

%% Jet / muon start of discussion sand
% Jets are `messy', right?
% Jets do tend to worse resolution --- they suffer from calorimeter noise
% (which varies with statistical properties of hadronic showers
% along with hardware effects), as well as variable
% Jets include neutral hadrons which cannot be tracked, jet clustering may
% incorrectly assign objects, and some hadrons can decay to neutrinos and soft
% muons which would go undetected, and calorimetry is inherently variable.
% This tendency, however, fails for very hard muons.


\begin{equation}
\label{eqn:searches_logistic}
\prob{S}{d} = \frac{1}{1 + e^{-\Delta(d)}}
.
\end{equation}
Physically, this is analogous to the Boltzmann distribution where the energy
of each model is its negative $\log$ probability, with unit inverse
temperature~\cite{
skilling2017david,
lecun-06,
pmlr-v2-ranzato07a
}.
With multiple models in play, it is also called the `softmax'
function.
