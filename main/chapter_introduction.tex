\setcounter{chapter}{-1}
\begin{singlespacing}
\chapter{Introduction}
\label{chapter:introduction}
%
\begin{epigraphs}
\qitem{%
\TODO{}}%
{\TODO{},
\textit{\TODO{}},
\TODO{}}
\end{epigraphs}
\end{singlespacing}

Data Intensive Science in High-Energy Physics

\section{Other work}
\TODO{This does not belong in an introduction and will be moved.}

\subsection{Higgs to tau-tau Nested Sampling}
Proof-of-concept implementation of the Nested Sampling algorithm~\cite{
skilling2004nested,
skilling2006nested,
skilling2010foundations
}
to classify events mediated by Higgs or Z bosons in their leptonic decays
through tau lepton pairs in \atlas.
In this scheme, Nested Sampling approximates likelihoods at given data for
approximate models of the Higgs and Z models by `marginalizing',
or integrating over, all freedoms in their production in LHC proton-proton
collisions.
Models of their processes were constructed from theory for the
helicity-dependent leptonic decays of tau leptons~\cite{Hays2014tau}
(with their two neutrinos integrated out analytically)
and Z bosons~\cite{Thomson2011EWK},
and parametrized from numerical parton distribution
functions~\cite{Buckley:2014ana} for the models' differing distributions of
longitudinal boosts.

Unlike the similar Matrix Element Method~\cite{
Fiedler:2010sg,
Gainer:2013iya,
PhysRevD.83.074010
},
which estimates likelihoods by fixing certain observed quantities and
integrating remaining freedoms with adaptive Monte Carlo algorithms~\cite{
Fiedler:2010sg,
Gainer:2013iya,
Lepage:1977sw,
Ohl:1998jn
},
with Nested Sampling we begin in ignorance of the data in each event and
gradually explore its constraints with random walks.
Euclidean coordinates projected to the surface of the unit ball,
in each of the respective rest frames of the heavy boson and tau leptons, allow
those walks to avoid the gimble lock of spherical polar coordinates, and are
used to describe Lorentz transformations to transform the observable leptons
to the lab frame.

This method worked in testing on smeared parton-level simulations from
\madgraph~\cite{Alwall:2014hca} and was presented in two conference posters.
\TODO{Should these posters be included as figures? Maybe appendix}
However, real events have substantial additional noise from QCD
radiation~\cite{PhysRevD.83.074010} and pileup, and this method of approximate,
parametrized modelling does not easily scale to include such extra, complicated
effects.
For this special case it also cost a few second of computing time per event,
which is not competitive with modern Machine Learning algorithms that, once
trained, can be rapidly evaluated over many events and should approximate
the same result.
Although this study shows that we \emph{can} use theoretical modelling
with theoretically motivated algorithms for practical classification, it does
not show that we \emph{should}.
Empirically motivated function approximators such as
Boosted Decision Trees~\cite{xgboost} or
Neural Networks~\cite{MurphyKevinP.2012Mlap} are more scalable to complicated
uses and more computationally efficient in evaluation.

\subsection{Testing symmetries in data}
Data-driven tests of physical symmetries are a speciality of the Cambridge
\atlas\ group.
Each test considers a theoretically motivated symmetry that states that
distributions of data should be identical when transformed under its operation.
This means that it can be possible to disprove a symmetry by searching only
for differences between data and transformed versions of those data, without
necessarily requiring other theoretical inputs.

% emus
Charge symmetry of electron-muon (`emu') pairs is a key example among these
tests.
Theoretically, it proposes that, in the absence of biases, data
from proton-proton collisions at the LHC would include equal distributions
of $e^+\mu^-$ and $e^-\mu^+$ events~\cite{Lester:2016qdv}.
Real data can have emu biases --- one clear (but minor) example is that our
detectors are made of matter containing negatively charged electrons, not
positrons, so high energy electrons positrons have subtly different rates of
energy loss in that detector matter.
However, \cite{Lester:2016qdv} argues that all biases at the LHC should
conspire to favour $e^-\mu^+$ events.
Therefore defining an asymmetry
\begin{equation}
A^\mathrm{emu} = \frac{
\sigma(e^+\mu^- + X)
}{
\sigma(e^-\mu^+ + X)
}
,
\end{equation}
one can predict $A^\mathrm{emu} \leq 1$ from the Standard Model at the LHC.
Negative values could be explained by experimental biases, but
$A^\mathrm{emu} > 1$ measured in data might indicate new physics, since it and
can be caused by models of new physics that might invoke R-parity violating
supersymmetry~\cite{Lester:2016qdv} or leptoquarks~\cite{EXOT-2018-29}.
After its extensive development~\cite{Brunt:2674708,Pacey:2747774},
this test has been performed in \atlas\ data, and found ``[n]o significant
model-independent evidence''~\cite{EXOT-2018-29} for $A^\mathrm{emu} > 1$.

% parity violation
Parity symmetry is a second example. Nature violates parity is as first shown
by the Wu experiment~\cite{PhysRev.105.1413} in the weak section, but unknown
effects could possibly cause unexpected new forms of parity asymmetry that
could be seen at the LHC. In particular, \TODO{}


Data-driven tests of symmetries:
\TODO{write, add some self-supervision references from wir}
Machine Learning algorithms to ~\cite{lester2021stressed}
~\cite{Lester:2019bso}
~\cite{tombs2021which}
~\cite{lester2022hunting}


% this comment activates enlarged spacing for the final paragraph lol
